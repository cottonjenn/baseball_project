[
  {
    "objectID": "Tutorial.html",
    "href": "Tutorial.html",
    "title": "Getting Started",
    "section": "",
    "text": "Here is how you use my package:\n\n# from final_project_demo import add\n\n# print(add(2, 19))\n\nhave code that actually runs to demonstrate the package actually runs\n\n# from final_project_demo import run_analysis_pipeline, run_cleaning_pipeling\n\n# run_cleaning_pipeling()\n# run_analysis_pipeline()\n\n\n# from bb_salaries import create_http_headers\n\n\n\n\n\n\n\n\npartner/master"
  },
  {
    "objectID": "Documentation.html",
    "href": "Documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "This goes through the process of each function for each step of our process of collecting the data, organizing it into a coherent dataset, and performing analysis.\nThe first part starts with the collection of baseball hitting statistics from 2018 - 2025 to get a large sample of current data to work with. Collecting salaries was more complicated so we will go over this after. All the data was collected through the site baseball-reference.com.\nThis first chunk will show the initial setup for the data collection:\n\n# import requests\n# import re\n# from bs4 import BeautifulSoup\n# import pandas as pd\n# import time\n\n# URLS = [\n#     'https://www.baseball-reference.com/leagues/majors/2018-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2019-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2020-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2021-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2022-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2023-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2024-standard-batting.shtml',\n#     'https://www.baseball-reference.com/leagues/majors/2025-standard-batting.shtml',\n# ]\n\n# HEADERS = {\n#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n# }\n\n# COLUMNS = [\n#     'Rk','Player','Age','Team','Lg','WAR','G','PA','AB','R','H','2B','3B','HR','RBI',\n#     'SB','CS','BB','SO','BA','OBP','SLG','OPS','OPS+','rOBA','Rbat+','TB','GIDP',\n#     'HBP','SH','SF','IBB','Pos','Awards'\n# ]\n\nThis chunk sets up where the data is going to be scraped from and which columns to use.\nThis second chunk is the function to actually scrape all the statistics data from those pages. It identifies where to scrape from, how many rows to scrape, and loops through the hitting dataset from each year from 2018 - 2025. It creates the dataframe for all the statistics.\n\n# def scrape_batting_data(urls):\n#     all_data = []\n\n#     for url in urls:\n#         year = int(re.search(r'/(\\d{4})', url).group(1))\n#         print(f\"\\nScraping {year}...\")\n\n#         r = requests.get(url, headers=HEADERS, timeout=15)\n#         r.raise_for_status()\n#         soup = BeautifulSoup(r.text, 'html.parser')\n\n#         div = soup.find('div', id='switcher_players_standard_batting')\n#         if not div:\n#             print(\"  Switcher div not found\")\n#             continue\n\n#         table = div.find('table', id='players_standard_batting')\n#         if not table:\n#             print(\"  Table not found\")\n#             continue\n\n#         tbody = table.find('tbody')\n#         if not tbody:\n#             print(\"  Tbody not found\")\n#             continue\n\n#         seen = set()\n\n#         for row in tbody.find_all('tr'):\n#             if row.get('class') and 'thead' in row.get('class'):\n#                 continue\n\n#             cells = row.find_all(['th', 'td'])\n#             if len(cells) &lt; 25:\n#                 continue\n\n#             player_cell = cells[1]\n#             player_name = player_cell.get_text(strip=True)\n#             team = cells[3].get_text(strip=True)\n\n#             if player_name in seen and team != 'TOT':\n#                 continue\n\n#             if team == 'TOT' or player_name not in seen:\n#                 seen.add(player_name)\n\n#                 row_dict = {'Year': year}\n\n#                 pos_map = {\n#                     0: 'Rk', 1: 'Player', 2: 'Age', 3: 'Team', 4: 'Lg',\n#                     5: 'WAR', 6: 'G', 7: 'PA', 8: 'AB', 9: 'R', 10: 'H',\n#                     11: '2B', 12: '3B', 13: 'HR', 14: 'RBI', 15: 'SB',\n#                     16: 'CS', 17: 'BB', 18: 'SO', 19: 'BA', 20: 'OBP',\n#                     21: 'SLG', 22: 'OPS', 23: 'OPS+', 24: 'rOBA',\n#                     25: 'Rbat+', 26: 'TB', 27: 'GIDP', 28: 'HBP',\n#                     29: 'SH', 30: 'SF', 31: 'IBB', 32: 'Pos', 33: 'Awards'\n#                 }\n\n#                 for idx, cell in enumerate(cells):\n#                     col_name = pos_map.get(idx)\n#                     if not col_name:\n#                         continue\n\n#                     if col_name == 'Player':\n#                         row_dict['Player'] = player_name\n#                         a = cell.find('a')\n#                         if a:\n#                             row_dict['Player_Link'] = (\n#                                 'https://www.baseball-reference.com' + a['href']\n#                             )\n#                     else:\n#                         row_dict[col_name] = cell.get_text(strip=True)\n\n#                 for col in COLUMNS:\n#                     row_dict.setdefault(col, '')\n\n#                 all_data.append(row_dict)\n\n#         print(f\"  {len(seen)} players scraped for {year}\")\n#         time.sleep(1.2)\n\n#     return all_data\n\nThis third function cleans the data and organizes it into a coherent dataframe. The function within it, get_batting_hand(), identifies whether each player is left handed, right handed, or bats with both hands, as we decided this would also be useful information in determining the overall value of a hitter.\n\n# def clean_batting_data(df):\n#     numeric_cols = [\n#         'Age','WAR','G','PA','AB','R','H','2B','3B','HR','RBI','SB','CS',\n#         'BB','SO','OPS+','TB','GIDP','HBP','SH','SF','IBB','Rbat+'\n#     ]\n\n#     for col in numeric_cols:\n#         df[col] = pd.to_numeric(df[col], errors='coerce')\n\n#     df = df[~df['Player'].str.contains('MLB Average', case=False, na=False)]\n#     df = df[df['PA'] &gt;= 100]\n\n#     def get_batting_hand(name):\n#         if name.endswith('*'):\n#             return 'both'\n#         elif name.endswith('#'):\n#             return 'left'\n#         elif name.endswith('?'):\n#             return 'unknown'\n#         return 'right'\n\n#     df['batting_hand'] = df['Player'].apply(get_batting_hand)\n#     df['Player'] = df['Player'].str.rstrip('*#?')\n\n#     df = df.drop(columns=['Awards'])\n#     df = df.sort_values(['Player', 'Year'])\n#     df = df.drop_duplicates(subset=['Player', 'Year'], keep='first')\n\n#     player_counts = df['Player'].value_counts()\n#     players_2plus = player_counts[player_counts &gt;= 3].index\n#     df = df[df['Player'].isin(players_2plus)]\n\n#     return df.reset_index(drop=True)\n\nThis main function actually implements the previous functions, scraping all the statistics according to how we wanted, to create a dataset of each mlb hitter from 2018 - 2025, filtered to players with over 100 plate appearances in at least one of the seasons.\n\n# def main():\n#     all_data = scrape_batting_data(URLS)\n\n#     df = pd.DataFrame(\n#         all_data,\n#         columns=['Year'] + COLUMNS + ['Player_Link']\n#     )\n\n#     mlb = clean_batting_data(df)\n\n#     mlb.to_csv('MLB_2018_2025_Cleaned.csv', index=False)\n#     print(\"\\nSaved: MLB_2018_2025_Cleaned.csv\")\n\n\n# # ==========================\n# # ENTRY POINT\n# # ==========================\n\n# if __name__ == \"__main__\":\n#     main()\n\nNow we will move on to the next part of the data collection, which is gathering the salary data. This was more complicated to collect, because there is no dataset that has each player’s salary data in one place. We had to build a scraper that iterated through the unique page of each player in the dataset within baseball-reference and scraped their salaries from 2018 - 2025. The first dataset had included the links to each of these players’ pages, so we utilized this to create a json file of unique player links for the scraper to iterate through, so that no players’ page would be accessed more than once. Each code chunk will be explained below.\n\n# def create_http_headers() -&gt; dict[str, str]:\n#     return {\n#         'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n#         'Accept-Language': 'en-US,en;q=0.5',\n#         'Accept-Encoding': 'gzip, deflate',\n#         'Connection': 'keep-alive',\n#         'Upgrade-Insecure-Requests': '1',\n#     }\n\nThis first function creates the headers that allows us to access the data and browser without being blocked.\n\n# def parse_salary_table_from_soup(soup: BeautifulSoup) -&gt; dict[int, int]:\n    \n#     salary_dict = {}\n   \n#     # First try to find the table normally\n#     salary_table = soup.find('table', {'id': 'br-salaries'})\n   \n#     # If not found, look for it in HTML comments (common pattern for baseball-reference)\n#     if not salary_table:\n#         comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n#         for comment in comments:\n#             if 'id=\"br-salaries\"' in comment:\n#                 comment_soup = BeautifulSoup(comment, 'html.parser')\n#                 salary_table = comment_soup.find('table', {'id': 'br-salaries'})\n#                 if salary_table:\n#                     break\n   \n#     if not salary_table:\n#         return salary_dict\n   \n#     rows = salary_table.find_all('tr')\n#     if not rows:\n#         return salary_dict\n   \n#     header_row = rows[0]\n#     headers = [th.get_text().strip() for th in header_row.find_all(['th', 'td'])]\n   \n#     try:\n#         year_col_idx = headers.index('Year')\n#         salary_col_idx = headers.index('Salary')\n#     except ValueError:\n#         return salary_dict\n   \n#     for row in rows[1:]:\n#         cells = row.find_all(['td', 'th'])\n#         if len(cells) &lt;= max(year_col_idx, salary_col_idx):\n#             continue\n           \n#         year_text = cells[year_col_idx].get_text().strip()\n#         salary_text = cells[salary_col_idx].get_text().strip()\n       \n#         if year_text.isdigit() and len(year_text) == 4:\n#             try:\n#                 year = int(year_text)\n#                 if 2018 &lt;= year &lt;= 2025:\n#                     if salary_text and salary_text.startswith('$'):\n#                         salary_clean = salary_text.replace(\"$\", \"\").replace(\",\", \"\").strip()\n#                         if salary_clean:\n#                             try:\n#                                 salary = int(salary_clean)\n#                                 salary_dict[year] = salary\n#                             except ValueError:\n#                                 pass\n#             except ValueError:\n#                 continue\n   \n#     return salary_dict\n\nThis second function is the one that finds and parses the salary data in each player’s page. It identifies where it is, finds the salaries from 2018-2025, and converts the data frmo string to integer form. One difficulty is that many, if not all, of the players’ salary data was within comments in the source code of their individual pages, so we had to account for this in our scraper.\n\n#   async def scrape_salary_from_url(url: str, session: aiohttp.ClientSession) -&gt; dict[int, int]:\n#     print(f\"Starting to scrape {url}.\")\n   \n#     try:\n#         async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:\n#             if response.status != 200:\n#                 print(f\"Failed to fetch {url}: HTTP {response.status}\")\n#                 return {}\n           \n#             html = await response.text()\n#             soup = BeautifulSoup(html, 'html.parser')\n#             salary_dict = parse_salary_table_from_soup(soup)\n           \n#             print(f\"Done scraping {url}. Found {len(salary_dict)} salary entries.\")\n#             return salary_dict\n           \n#     except asyncio.TimeoutError:\n#         print(f\"Timeout while scraping {url}\")\n#         return {}\n#     except aiohttp.ClientError as e:\n#         print(f\"HTTP error while scraping {url}: {e}\")\n#         return {}\n#     except Exception as e:\n#         print(f\"Unexpected error while scraping {url}: {e}\")\n#         return {}\n\nThis function downloads each player’s page asynchonously and extracts the salary data.\n\n# def extract_unique_links(csv_path: str, output_json_path: str) -&gt; None:\n#     \"\"\"Extract unique player links from CSV and save as JSON\"\"\"\n#     df = pd.read_csv(csv_path)\n   \n#     url_to_player = df.dropna(subset=['Player_Link']).drop_duplicates(subset=['Player_Link']).set_index('Player_Link')['Player'].to_dict()\n#     unique_urls = df['Player_Link'].dropna().unique().tolist()\n#     links_with_ids = [{\"id\": i + 1, \"url\": url, \"player\": url_to_player[url]} for i, url in enumerate(unique_urls)]\n   \n#     with open(output_json_path, 'w') as f:\n#         json.dump(links_with_ids, f, indent=2)\n\nThis function extracts the links out of the dataset of player links, to create a list of each unique players’ page links (each player’s link appearing once). This allows our function to easily loop through without accessing players’ pages multiple times, as many players played multiple seasons over the 2018 - 2025 period.\n\n# def scrape_with_cloudscraper(url: str, scraper) -&gt; dict[int, int]:\n#     print(f\"Scraping {url}\")\n#     try:\n#         html = scraper.get(url, timeout=30).text\n#         soup = BeautifulSoup(html, 'html.parser')\n#         return parse_salary_table_from_soup(soup)\n#     except Exception as e:\n#         print(f\"Failed {url}: {e}\")\n#         return {}\n\nThis function utilizes cloudscraper to bypass Cloudflare in baseball-reference, ebcause cloudflare was blocking any attempts to scrape data from the pages otherwise.\n\n# def churn_with_cloudscraper():\n#     scraper = cloudscraper.create_scraper(\n#         browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False},\n#         delay=10\n#     )\n\n#     with open(\"unique_links.json\", \"r\") as f:\n#         links = json.load(f)\n\n#     try:\n#         with open(\"salaries.json\", \"r\") as f:\n#             existing = json.load(f)\n#     except:\n#         existing = []\n\n#     existing_ids = {x['id'] for x in existing}\n#     remaining = [l for l in links if l['id'] not in existing_ids]\n\n#     results = {e['id']: e for e in existing}\n\n#     for link in remaining:\n#         salary_data = scrape_with_cloudscraper(link['url'], scraper)\n#         results[link['id']] = {\n#             \"id\": link['id'],\n#             \"player\": link['player'],\n#             \"salaries\": salary_data\n#         }\n\n        \n#         with open(\"salaries.json\", \"w\") as f:\n#             json.dump(sorted(results.values(), key=lambda x: x['id']), f, indent=2)\n\n#         print(f\"Success: Saved {link['player']} → {len(salary_data)} years\")\n#         time.sleep(4)  \n\n#     print(\"All done!\")\n\nThis function organizes everything together to scrape the data, utilizing cloudscraper, laoding all the urls, saving progress to be able to pause and resume later, iterates over each link, limits rates, and outputs the data to salaries.json."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 386 Final Project",
    "section": "",
    "text": "MLB Baseball Performance vs Salary\nBy: Isaac Miller and Jenna Worthen\nFor our final project, we decided to focus on the effect of salaries on a baseball player’s productivity. One thing that seems to be a common occurrence among baseball players (and the sporting industry as a whole) is that as soon as they sign a big contract of guaranteed money, their production on the field seems to regress. No example of this is more apparent than that of Anthony Rendon, who after almost winning MVP and leading his team to a world series title in 2019, immediately seemed to regress after signing a massive contract with the Los Angeles Angels. This, along with several other notable examples, prompted us to ask the question: Does a player’s salary negatively impact their play?\n\n\n\nAnthony Rendon by Larry Brown Sports\n\n\nWe will built a python package that will enable users replicate how we scraped and analyzed baseball data from baseball-reference.com. View the links below to see our documentation and results!\nDocumentation here.\nGet started here.\nReview the technical report.\nHard Skills we demonstrate in Python: webscraping (streamlit and BeautifulSoup), modeling with multiple linear regression, data cleaning and wrangling, data visualization"
  },
  {
    "objectID": "TechnicalReport.html",
    "href": "TechnicalReport.html",
    "title": "Technical Report",
    "section": "",
    "text": "Overall for this project, we wanted to see if there was a connection between a baseball player’s salary and their on-field performance. We found that there was a connection overall, that as their salary went up their perfomance seemed to go up, overall. While it is imporatnt for teams to thoroughly evaluate each decision they make about which players to sign, overall offering a large contract to a star palyer does not seem to diminish their play. As players get paid more, their perfomrance genreally tends to trend upward."
  },
  {
    "objectID": "TechnicalReport.html#executive-summary",
    "href": "TechnicalReport.html#executive-summary",
    "title": "Technical Report",
    "section": "",
    "text": "Overall for this project, we wanted to see if there was a connection between a baseball player’s salary and their on-field performance. We found that there was a connection overall, that as their salary went up their perfomance seemed to go up, overall. While it is imporatnt for teams to thoroughly evaluate each decision they make about which players to sign, overall offering a large contract to a star palyer does not seem to diminish their play. As players get paid more, their perfomrance genreally tends to trend upward."
  },
  {
    "objectID": "TechnicalReport.html#project-context",
    "href": "TechnicalReport.html#project-context",
    "title": "Technical Report",
    "section": "Project Context",
    "text": "Project Context\nTODO: Describe the motivation, stakeholders, and success criteria for this analysis. The motivation behind this was the idea that certian mlb players seem to deteriorate in on-field perfoermace after signing a large free agent contract. We suspected that when players are given enormous sums of money to play, they could develop a sense of security and feel less of a necessity to perform to their whole abilities. These findings could be valuable to major league baseball teams, who could alter their strategies in recruiting and signing free agent players so their teams can play to their full potential. Success criteria for this analysis would be making a significant discovery about this through analysing the data, and in our case gaining sufficient evidence proving our suspicion about player performance and salaries. ## Data Sources All data we collected were from various sections of the site, baseball-reference.com. - Primary dataset: TODO - name and link We collected data from the hitter datasets in baseball reference for each year 2018-2025. This was combined with scraped salary data in other sections of the site for our overall dataset. - Supplementary data: TODO - describe any external references - Data access notes: TODO - mention licenses or refresh cadence Baseball-reference allows this according to their robots.txt. ## Methodology\n\nData acquisition: TODO - outline collection scripts/APIs We collected data from the hitter datasets in baseball reference for each year 2018-2025. We filtered so only players with 100+ Palte Appearances appeared in the datasets. Salary Data was not available in a large dataset, so we had to build a scraper to loop through each player’s individual page and scrape the salary data, creatign a dataset with just the salary data. We combined these datasets to create our overall dataset for hitters’ statistics and salaries.\nCleaning pipeline: TODO - summarize transformations and validations implemented\nAnalysis workflow: TODO - describe statistical or modeling techniques\nTooling: TODO - list packages, environments, and reproducibility steps"
  },
  {
    "objectID": "TechnicalReport.html#results-diagnostics",
    "href": "TechnicalReport.html#results-diagnostics",
    "title": "Technical Report",
    "section": "Results & Diagnostics",
    "text": "Results & Diagnostics\nTODO: Summarize the main metrics, charts, or model diagnostics produced. Include links to figures or tables once available."
  },
  {
    "objectID": "TechnicalReport.html#discussion-next-steps",
    "href": "TechnicalReport.html#discussion-next-steps",
    "title": "Technical Report",
    "section": "Discussion & Next Steps",
    "text": "Discussion & Next Steps\nTODO: Interpret the results, note limitations, and capture open questions or future experiments."
  }
]